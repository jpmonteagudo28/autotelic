{
  "hash": "16dcc600bab644f6f13046cef808ce65",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapters 1–2\"\ndate: now\nformat:\n  html:\n    css: /learn/style.css\n    code-fold: false\n    code-link: true\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: false\ncitation: false\ndraft: false\nhaiku:\n  - Thought you were normal\n  - but Bayes says you're different;\n  - heart's torn, you or him.\nlightbox: true\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n\n## Chapter 1\n\n### Of Models and Monsters\n\n-   The Golem of Prague - Jewish mythical creature that is awakened and\n    commanded by truth but lacks wisdom. Its lack of discernment renders\n    him dangerous and vulnerable to the work of evil agents.\n\n-   Statistical models are like golems, set up and commanded by truth to\n    obey without complaints but unable to discern context and\n    usefulness.\n\n-   A clear and urgent need for a unified theory of statistics that\n    allows for flexibility and freedom in designing, building and\n    refining special-purpose statistical procedures. Classic statistics\n    tests are rigid and applicable to only a handful of procedures; but\n    even those tests that offer more flexibility, like **ordinary linear\n    regression** relies on strong assumptions that lead to catastrophic\n    results if violated.\n\n### The Monster of Deductive Falsification (DF)\n\n-   DF is impossible in nearly every scientific context:\n    -   NHST is not falsificationist since it doesn't falsify the\n        research hypothesis, but the null of no effect.\n    -   Hypothesis are not models. One hypothesis may be represented by\n        many different models, and nay given statistical model may\n        correspond to more than one hypothesis.\n    -   Possible solution: Compare more than one model. All statistical\n        tests are also models.\n\n### The Weapons of the Fight\n\n-   Bayesian data analysis\n    -   count the number of ways things can happen, according to your\n        assumptions.\n    -   Randomness is a property of information, not the world.\n-   Multilevel models\n    -   The Earth stands on the back of an elephant, which stands on the\n        back of a turtle. Where does the turtle stand? “It's turtles all\n        the way down”…for us it's parameters all the way down.\n    -   **Multilevel regression deserves to be the default form of\n        regression**\n    -   To adjust estimates for repeat sampling\n    -   To adjust estimates for imbalance in sampling\n    -   To study variation\n    -   To avoid averaging\n-   Model comparison using uniform criteria\n    -   Model comparison based on future predictive accuracy\n\n### Where to Go from Here?\n\n-   **Chapter 2 and 3** - foundation Bayesian inference tools\n-   **Chapter 4 - 7** - build multiple linear regression as Bayesian\n    tools.\n-   **Chapter 8 - 11** - generalized linear models:\n    -   Markov Chain Monte Carlo (MCMC)\n    -   Maximum Entropy\n    -   Model details\n-   **Chapter 12 - 14** - Multilevel models, linear and generalized,\n    missing data, measurement error, and spatial correlation.\n-   **Chapter 15** - returns to some of the issues raised in Chapter 1.\n\n## Chapter 2 - Small Worlds and Large Worlds\n\nChristopher Colombo miscalculated the diameter of the earth– he thought\nit was smaller (30,000 km instead of 40,000), landed in the Bahamas and\ndiscover a new world.\n\n-   The model is the small world, and we hope to deploy it in the large\n    world of reality and be useful.\n-   Bayesian models are optimal in small worlds, but need to be\n    demonstrated rather than logically deduced in large worlds.\n\n### The garden of forking data\n\n-   Consider everything that could have happened.\n\n1.  Count the possibilities\n2.  Use prior information\n    -   from previous data\n    -   knowledge of how the process works\n    -   Act as if you had prior information\n        -   multiply the prior count by the new count\n        -   When we have previous information suggesting there are\n            $W_{prior}$ ways for a conjecture to produce a previous\n            observation $D_{prior}$ and,\n        -   We acquire new observations $D_{new}$ then the same\n            conjecture can produce in $W_{new}$ ways\n        -   the number of ways the conjecture can account for\n            $D_{prior}$ and $D_{new}$ is the product of\n            $W_{prior} \\times W_{new}$.\n    -   from count to probability\n$$\n\\begin{split}\n    \\text{Plausibility of} X \\text{ after observing } x \\\\\n    \\propto \\text{ ways } X \\text{ can produce } x \\\\\n    \\times \\text{ prior plausibility of } X\n\\end{split}\n$$\n        -   standardize the probabilities (divide by the sum of\n            products)\n\nA few things to define:   \n\n  * the **parameter** is the conjectured proportion of blue marbles. \n  * the **likelihood** is the relative number of ways that a *p* value can produce the data \n  * the **prior probability** is the prior plausibility of any specific *p* values \n  * the **posterior probability** is the new, updated plausibility of any specific *p*\n\n### Building a model\n\nWorking with a toy model to get an idea of how Bayesian inference works\n\n>Suppose you have a globe representing our planet, the Earth. This\nversion of the world is small enough to hold in your hands. You are\ncurious how much of the surface is covered in water. You adopt the\nfollowing strategy: You will toss the globe up in the air. When you\ncatch it, you will record whether or not the surface under your right\nindex finger is water or land. Then you toss the globe up in the air again and repeat the procedure\n\nbut first, there are assumptions that constitute the model:\n\n1.  Data story: motivate the model by narrating how the data might arise\n    -   How the data came to be\n        -   Descriptive story\n        -   Causal story \\_ Involves restating:\n            -   the true proportion of the conjecture, *p*\n            -   the probability of producing an alternative conjecture,\n                $1-p$\n            -   independence of conjectures\n2.  Update: Educate your model by feeding it the data\n    -   Begins with prior plausibilities\n    -   Updates them in light of the data\n    -   A model can be updated forward-in-time, backwards or\n        all-at-once. It can mathematically divide out the observations\n        to infer the previous plausibility curve.\n    -   A bad prior leads to misleading results just like bad estimators\n        in Fisherian inference lead to bad results.\n3.  Evaluate: All statistical models require supervision, leading\n    possibly to model revision.\n    -   Real world data must be accurately described to the model.\n    -   Models are never true to reality so there's no point in checking\n        the truth of the model.\n    -   You either fail or succeed at recognizing the falseness of the\n        model.\n    -   Check adequacy of the model, not its truth.\n\n*Likelihood*: the probability of any possible observation, for any\npossible state of the small world.\n\n$$\nP(w|n,p) = \\frac{n!}{w!(n - w)!}p^w(1-p)^{(n-w)}\n$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Only two outcomes, W and L so p = 0.5, n = 9\nw <- dbinom(6,size = 9, prob = 0.5)\nprint(w)\n#> [1] 0.164\n```\n:::\n\n\n\n*Parameters*: the adjustable inputs ($n,p,w$)\n\n*Priors*: for every parameter, there must be a prior probability and constrain the parameters to reasonable ranges. \n\n*Posteriors*: unique set of estimates for every combination of data, likelihood, parameters and priors that takes the form of the probability of the parameters, conditional on the data $P(p|n,w)$, defined by **Bayes theorem**\n\n$$\nP(w,p) = P(w|n,p)P(p)\nP(w,p) = P(p|w,n)P(w)\n$$\nBayes' theorem is just the posterior probability of p given w. The product of the likelihood and prior, divided by $P(w)$, the average likelihood over the prior^[$P(w) = E(P(w|p)) = \\int P(w|p)P(p)\\partial p$].\n$$\nP(p|w,n) = \\frac{P(w|p)P(p)}{P(w)}\n$$\n\n### Making the model go\n\nThe core of the model, its motor, conditions the prior on the data, which can be done without forcing simple and rigid forms of prior that are easy to work with in three of many ways:\n\n* Grid approximation  \n    - continuous parameters\n    - only useful as a teaching tool\n    - Scales very poorly\n    - define the grid: how many points to use in estimating the posterior, then make a list of parameters in the grid.\n    - compute the value of the prior at each parameter value on the grid\n    - compute the unstandardized posterior at each parameter value\n    - standardize the posterior\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Setting global parameter for base plots\npar(bg = \"#Fffff2\")\n\n# Define the grid\np_grid <- seq(0,1,length.out = 20)\n\n# define the prior\nprior <- rep(1,20)\n\n# compute the likelihood @ each value on the grid\nlikelihood <- dbinom(6,9,prob = p_grid)\n\n# compute product of likelihood and prior\nunstd_posterior <- likelihood * prior\n\n# standardize the posterior\nposterior <- unstd_posterior/sum(unstd_posterior)\n\nplot(p_grid,posterior, type = \"b\",\n     bty= \"n\",\n     pch = 16,\n     col = \"#7c225c\",\n     family = font_family,\n     xlab = \"probability of water\",\n     ylab = \"posterior probability\")\nmtext(\"20 points\", family = font_family)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\n\n# You can try sparser (<20) or denser grids (>100). The correct density \n# for your grid depends on how accurate you want your approximation to be.\n```\n:::\n\n\n\n* Quadratic approximation\n    - Called quadratic approximation because the log of a normal distribution forms a parabola\n    - the posterior distribution can be approximated by a Gaussian, under general conditions\n    - We can make use of only two parameters, its center and spread\n    - Find the peak of the posterior (its center), then estimate the curvature near the peak\n    - compute quadratic approximation using this estimate\n    - as *n* increases, the quadratic approximation gets better\n    - equivalent to maximum likelihood estimate (MLE) and its standard error\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Setting global parameter for base plots\npar(bg = \"#Fffff2\")\n\nlibrary(rethinking)\n\nglobe_quad_approx <- map(\n  \n  alist(\n    \n    w ~ dbinom(9,p), # binomial likelihood\n    p ~ dunif(0,1) # uniform prior\n  ),\n  data = list(w = 6)\n)\n\n# display summary of quad. approximation\n\nprecis(globe_quad_approx)\n#>    mean    sd  5.5% 94.5%\n#> p 0.667 0.157 0.416 0.918\n\n# Compare the quad. approximation to the analytical solution\n\nw <- 6\nn <- 9\n\n# exact posterior\ncurve(\n  dbeta(x,\n            w+1,\n            n - w+1),\n      0,1,\n  xlab = \"proportion water\",\n  ylab = \"density\",\n  bty = \"n\",\n  family = font_family)\n\n# quad. approximation\ncurve(dnorm(x,0.67,0.16), lty = 2, add = TRUE,\n      xlab = \"proportion water\",\n      ylab = \"density\",\n       col = \"#7c225c\",\n      bty = \"n\",\n      family = font_family)\naxis(1)\naxis(2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n* MCMC\n    - models for which grid or quadratic approximation are always satisfactory. \n    - doesn't compute or approximates the posterior but draws samples from the posterior. \n    - Results in a collection of parameters and its frequencies which correspond to the posterior plausibilities.\n\n\n## Practice Problems\n\n#### 2M1\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n## 2M1(i,ii,iii)\n\n# Create function to compute prior, likelihood, posterior and print posterior plot\ncreate_posterior_plot <- function(trials, \n                                  successes,\n                                  grid_points,\n                                  ...,\n                                  mtext_label = NULL,\n                                  xlab = NULL, \n                                  ylab = NULL){\n  \n  args <- list(...)\n  \n  # Define the grid\n  p_grid <- seq(0,1, length.out = grid_points)\n  \n  # Define the prior\n  prior <- if (\"prior\" %in% names(args)) args$prior else rep(1, grid_points)\n  \n  # Ensure the prior length matches grid_points\n  if (length(prior) != grid_points) {\n    stop(\"Length of prior must match grid_points\")\n  }\n  \n  # Compute the likelihood for each value in the grid\n  likelihood <- dbinom(successes,trials,prob = p_grid)\n  \n  # Compute unstandardized posterior\n  unstd_posterior <- likelihood * prior\n  \n  # Standardize posterior\n  posterior <- unstd_posterior/sum(unstd_posterior)\n  \n  plot(p_grid, posterior, type = \"b\",\n       bty = \"n\",\n         xlab = xlab,\n         ylab = ylab,\n       pch = 16, \n       col = \"#7c225c\",\n       family = font_family)\n    mtext(mtext_label, \n          family = font_family)\n    \n    # Placement of x-axis\n    axis(1, lwd = 0.5)\n    # Placement of y-axis\n    axis(2, lwd = 0.5)\n}\n\npar(mfrow = c(1,3), \n    bg = \"#Fffff2\")\n\ncreate_posterior_plot(3,3,20,\n                      mtext_label = \"100 %success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(4,3,20,\n                      mtext_label = \"75% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(7,5,20,\n                      mtext_label = \"71% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/medium-problems-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n#### 2M2\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Setting global parameter for base plots\npar(mfrow = c(1,3), bg = \"#Fffff2\")\n\n# 2M2 - assume a prior equal to zero when p < 0.5 \n# and a positive constant when p >= 0.5\np_grid <- seq(0,1,length.out = 20)\n\ncreate_posterior_plot(3,3,\n                      grid_points = 20,\n                      prior = ifelse(p_grid < 0.5,0,1),\n                      mtext_label = \"100% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(4,3,\n                      grid_points = 20,\n                      prior = ifelse(p_grid < 0.5,0,1),\n                      mtext_label = \"75% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(7,5,\n                      grid_points = 20,\n                      prior = ifelse(p_grid < 0.5,0,1),\n                      mtext_label = \"71% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/medium-probs-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n#### 2M3\n\nShow that the posterior probability of tossing the Earth given a land observation is $P(E|L) = .23$ if the probability of observing water on the Earth is $P(W|E) = .7$ and on Mars $P(W|M) = 0$. Assume $P(E)  =.5$\n\n$$\n\\begin{split}\nL = Land, W = Water, E = Earth, M = Mars \\\\\n\\\\\nP(L|E) = 0.3 \\\\\nP(E) = 0.5 \\\\\nP(M) = 0.5 \\\\\nP(L|M) = 1 \\\\ \\\\\nP(E|L) = \\frac{P(L|E)P(E)}{P(L)}\\\\ \\\\\nP(E|L) = \\frac{.3\\times .5}{(0.3 \\times .5 + 1\\times .5)}\\\\ \\\\\nP(E|L) = .231\n\\end{split}\n$$\n\n#### 2M4\n\nOne deck of cards with three cards. One card has two black sides, another has one black and one white side. The third one has two white sides. Show that $P(B2|B1) = \\frac{2}{3}$ using the counting method. \n\n$$\n\\begin{split}\nP(B^2) = P(B_{w}) = P(W^2) = 1/3 \\\\ \\\\\nP(B) = P(W) = 1/2 \\\\ \\\\\nP(B_{2}|P_{1}) = \\frac{P(B^2 \\cap B_{1})}{P(B_{t})} \\\\ \\\\\nP(B_{2}|P_{1}) = \\frac{0.3333}{(0.333 \\times 0.5 + 0.333 \\times 1 + 0.333 \\times 0)} \\\\ \\\\\nP(B_{2}|P_{1}) = 0.667 = \\frac{2}{3}\n\\end{split}\n$$\n\n#### 2M5\n\nSimilar scenario but 4 cards and $P(B^2) = 1/2$. What's the probability of getting a second black card after getting one black side. \n\n$$\n\\begin{split}\nP(B^2) = 1/2 \\\\ \\\\\nP(B_{w}) = P(W^2) = 1/4 \\\\ \\\\\nP(B) = 5/8 \\\\ \\\\\nP(W) = 3/8 \\\\ \\\\\nP(B_{2}|P_{1}) = \\frac{P(B^2 \\cap B_{1})}{P(B_{t})} \\\\ \\\\\nP(B_{2}|P_{1}) = \\frac{0.50}{0.625} \\\\ \\\\\nP(B_{2}|P_{1}) = 0.80 = \\frac{4}{5}\n\\end{split}\n$$\n\n#### 2H1\n\nInitially, had trouble seeing that I only had to use the law of total probability to get the total, unconditioned probability of twins $P(T)$. Once I figured that out, finding the probability of a second twin birth $P(T_{2}|T_{1})$ was easy to find.\n\n\\begin{split}\nP(T|T) = x \\\\ \\\\\nP(B_{1}) = P(B_{2}) = 0.5 \\\\ \\\\\nP(T|B_{1}) = 0.10 \\\\ \\\\\nP(T|B_{2}) = 0.20 \\\\ \\\\\n\\text{Total probability of twins} \\\\\nP(T) = P(T|B_{1})P(B_{1}) + P(T|B_{2})P(B_{2}) \\\\ \\\\\nP(T) = 0.10 \\times 0.50 + 0.20 \\times 0.50 = 0.15 \\\\ \\\\\n\n\\text{Probability of observing first species given a twin birth} \\\\\nP(B_{1}|T) = \\frac{P(T|B_{1})P(B)}{P(T)} = 0.333\\\\ \\\\\nP(B_{2}|T) = 1 - P(\\bar{B_{2}}) = 0.667 \\\\ \\\\\n\\text{Probability of a second twin birth regardless of species} \\\\\nP(T_{2}|T_{1}) = P(T|B_{1})P(B_{1}|T) + P(T|B_{2})P(B_{2}|T) \\\\ \\\\\nP(T_{2}|T_{1}) = 0.10 \\times 0.333 + 0.20 \\times 0.667 = 0.167\n\\end{split}\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}