[
  {
    "objectID": "learn/advanceR/advanceR/chap1_3/index.html",
    "href": "learn/advanceR/advanceR/chap1_3/index.html",
    "title": "Chapters 1–3",
    "section": "",
    "text": "If you’d like to know your way around the programming environment, you first have to understand how R’s OOP works. Binding is one of the most basic functions performed in R so let’s start there:\n\nx &lt;- (1:6)\nprint(x)\n#| [1] 1 2 3 4 5 6\n\nR assigns values to a name, and not vice-versa. This code is creating an object, a vector with 6 elements. This object is then bound to the name x– which is also a variable. The name acts as a reference to the values 1 through 6. This concept is clearly demonstrated by binding the existing values in x to y. This action doesn’t create a copy of (1:6), but binds the same values to a new name y. This doesn’t mean that you won’t see two names in your environment, instead, it shows that y is also referencing (1:6).\n\ny &lt;- x\n\nHow do we know this? Well, we can access the object’s identifier to make sure both x and y point to the same object (1:6).\n\nprint(lobstr::obj_addr(x))\n#| [1] \"0x2c05d4d5bd0\"\nprint(lobstr::obj_addr(y))\n#| [1] \"0x2c05d4d5bd0\"\n\n## Another example of this behavior\nprint(lobstr::obj_addr(mean))\n#| [1] \"0x2c058c0d440\"\nprint(lobstr::obj_addr(base::mean))\n#| [1] \"0x2c058c0d440\"\nprint(lobstr::obj_addr(match.fun(\"mean\")))\n#| [1] \"0x2c058c0d440\""
  },
  {
    "objectID": "learn/advanceR/advanceR/chap1_3/index.html#whats-in-a-name",
    "href": "learn/advanceR/advanceR/chap1_3/index.html#whats-in-a-name",
    "title": "Chapters 1–3",
    "section": "",
    "text": "If you’d like to know your way around the programming environment, you first have to understand how R’s OOP works. Binding is one of the most basic functions performed in R so let’s start there:\n\nx &lt;- (1:6)\nprint(x)\n#| [1] 1 2 3 4 5 6\n\nR assigns values to a name, and not vice-versa. This code is creating an object, a vector with 6 elements. This object is then bound to the name x– which is also a variable. The name acts as a reference to the values 1 through 6. This concept is clearly demonstrated by binding the existing values in x to y. This action doesn’t create a copy of (1:6), but binds the same values to a new name y. This doesn’t mean that you won’t see two names in your environment, instead, it shows that y is also referencing (1:6).\n\ny &lt;- x\n\nHow do we know this? Well, we can access the object’s identifier to make sure both x and y point to the same object (1:6).\n\nprint(lobstr::obj_addr(x))\n#| [1] \"0x2c05d4d5bd0\"\nprint(lobstr::obj_addr(y))\n#| [1] \"0x2c05d4d5bd0\"\n\n## Another example of this behavior\nprint(lobstr::obj_addr(mean))\n#| [1] \"0x2c058c0d440\"\nprint(lobstr::obj_addr(base::mean))\n#| [1] \"0x2c058c0d440\"\nprint(lobstr::obj_addr(match.fun(\"mean\")))\n#| [1] \"0x2c058c0d440\""
  },
  {
    "objectID": "learn/advanceR/advanceR/chap1_3/index.html#changes-to-a-name",
    "href": "learn/advanceR/advanceR/chap1_3/index.html#changes-to-a-name",
    "title": "Chapters 1–3",
    "section": "Changes to a name",
    "text": "Changes to a name\nThe naming possibilities seem endless, but they’re actually limited. Always use syntactic names that consists of letters, digits, . and _. You can’t begin with _ or a digit, and can’t use any reserved words like TRUE, FALSE,NULL,if, and function1. To override these rules, you have to surround the name in back–ticks.\n\n`function` &lt;- c(1:3,\"ay\", \"caramba\")\nprint(`function`)\n#| [1] \"1\"       \"2\"       \"3\"       \"ay\"      \"caramba\"\n\n`if` &lt;- 1e+7\nprint(`if`)\n#| [1] 1e+07\n\nWhat happens when I modify y? Do I get a new object or the same object, \"0x1c7fb8fc418\" with new values? A new object is created, but the original object (1;6), did not change2. R simply created a new object with one value changed, and rebound y to it.\n\nx &lt;- 1:6\ny &lt;- x\ncat(tracemem(y), \"\\n\")\n#&gt; &lt;0000023889E515A0&gt;\n\n# Modified object\ny[[6]] &lt;- 10\n\n#&gt; tracemem[0x0000023889e515a0 -&gt; 0x0000023891546ef8]: \n#&gt; tracemem[0x0000023891546ef8 -&gt; 0x0000023890d854f8]: \n\nuntracemem(y) # to stop tracing y\n\n# Using a function to test this concept\nf &lt;- function(a){\n  a\n}\n\ncat(tracemem(x), \"\\n\")\n\nz &lt;- f(x)\n# no copy made. Bound z to (1:6)\n\nuntracemem(x)\n\nWe just performed copy-on modify3 and, hopefully, realized that R objects are immutable– once created, it will stay unchanged4.\nLists\nLists store elements that point to a specific value, just like variables; however, R uses shallow copy–on modify when binding values to a new list. In other words, the list object and its bindings are copied, but not the values to which the elements are pointing.\n\nl1 &lt;- list(c(1:4),\"a\")\nlobstr::obj_addr(l1)\n#| [1] \"0x2c05f625418\"\n\nl2 &lt;- l1\nlobstr::obj_addr(l2)\n#| [1] \"0x2c05f625418\"\n\n# Modify l2\nl2[[3]] &lt;- 25\nlobstr::obj_addr(l2)\n#| [1] \"0x2c05ee6e938\"\n\n## Let's look at values shared across lists\nlobstr::ref(l1,l2)\n#| █ [1:0x2c05f625418] &lt;list&gt; \n#| ├─[2:0x2c05f625518] &lt;int&gt; \n#| └─[3:0x2c05d9b8660] &lt;chr&gt; \n#|  \n#| █ [4:0x2c05ee6e938] &lt;list&gt; \n#| ├─[2:0x2c05f625518] \n#| ├─[3:0x2c05d9b8660] \n#| └─[5:0x2c05d9b84d8] &lt;dbl&gt;\n\nData frames\nData frames are lists of vectors so modifying a column– one list– will not affect the binding of the other columns. However, modifying a row forces R to copy every column in the data frame.\n\nd1 &lt;- data.frame(x = (1:6), y = (7:12))\nlobstr::obj_addr(d1)\n#| [1] \"0x2c05ee95ad8\"\n\nd2 &lt;- d1\nlobstr::obj_addr(d2)\n#| [1] \"0x2c05ee95ad8\"\n\n# Modify one column\nd2[,2] &lt;- d2[,2] * 3\nref(d1,d2)\n#| █ [1:0x2c05ee95ad8] &lt;df[,2]&gt; \n#| ├─x = [2:0x2c05e05aec8] &lt;int&gt; \n#| └─y = [3:0x2c05e05adb0] &lt;int&gt; \n#|  \n#| █ [4:0x2c05f58e078] &lt;df[,2]&gt; \n#| ├─x = [2:0x2c05e05aec8] \n#| └─y = [5:0x2c05f740238] &lt;dbl&gt;\n\n# Modify one row\nd3 &lt;- d1\nd3[1,]&lt;- d3[1,] *3\nref(d1,d3)\n#| █ [1:0x2c05ee95ad8] &lt;df[,2]&gt; \n#| ├─x = [2:0x2c05e05aec8] &lt;int&gt; \n#| └─y = [3:0x2c05e05adb0] &lt;int&gt; \n#|  \n#| █ [4:0x2c05efb20c8] &lt;df[,2]&gt; \n#| ├─x = [5:0x2c05f0e2268] &lt;dbl&gt; \n#| └─y = [6:0x2c05f0e20a8] &lt;dbl&gt;\n\nCharacter vectors\nR uses a global string pool where each element in a character vector is a pointer to a unique string in the pool.\n\nchar &lt;- c(\"a\",\"a\",\"abc\",\"d\")\nlobstr::ref(char, character = T)\n#| █ [1:0x2c05f57bc58] &lt;chr&gt; \n#| ├─[2:0x2c058192780] &lt;string: \"a\"&gt; \n#| ├─[2:0x2c058192780] \n#| ├─[3:0x2c05e493490] &lt;string: \"abc\"&gt; \n#| └─[4:0x2c0586af9c0] &lt;string: \"d\"&gt;\n\n\n\n\n\n\n\n\n\nA little bit of functions\nLet’s see how R’s binding behavior affects performance by using two simple functions and comparing the speed as the number of columns increase.\n\n# Define function to create random data\nmake.rand &lt;- function(nrow,ncol){\n  set.seed(2899)\n  ran.matrix &lt;- matrix(runif(nrow * ncol)\n                       ,nrow = nrow)\n  as.data.frame(ran.matrix)\n}\n\n# Create function to perform simple operation on:\n# Data frames\nsum.df &lt;- function(x,means){\n  means &lt;- vapply(x,mean,numeric(1))\n  for(i in seq_along(means)){\n    x[[i]] &lt;- x[[i]] + means[[i]]\n  }\n  x\n}\n\n# and lists\nsum.list &lt;- function(x,means){\n  x &lt;- as.list(x)\n  x &lt;- sum.df(x,means)\n  list2DF(x) # using list2DF to level performance of both functions\n}\n\nmake.rand(2,2) # testing function with 2×2 matrix\n#|      V1     V2\n#| 1 0.250 0.7588\n#| 2 0.422 0.0154\n\n\n\n\n\n\n\nAs a rule of thumbs, initialize vectors outside of loops. Move the computation of the means vector inside the for loop, and you will notice a 17,000% increase in computation time.\n\n\n\nI’ve created two functions to wrap the mean addition of a data frame and a list of randomly generated data. Next, I’ll create a function to evaluate performance of each operation on dfs and lists and thus the overhead of for loops5.\n\n# Function to create ran. data, calc. mean add., and eval. performance.\nbench.means &lt;- function(ncol){\n  df &lt;- make.rand(nrow = 1e4,ncol = ncol)\n  means &lt;- vapply(df,mean,numeric(1), USE.NAMES = F)\n  \n  bench::mark(\"data frame\" = sum.df(df,means),\n              \"list\" = sum.list(df,means),\n              time_unit = \"ms\")\n}\n\nbench.means(1) # testing function w. 1 column\n#| # A tibble: 2 × 6\n#|   expression    min median `itr/sec` mem_alloc `gc/sec`\n#|   &lt;bch:expr&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#| 1 data frame 0.0465  0.101     8674.   105.2KB     12.2\n#| 2 list       0.0628  0.125     6680.    93.3KB     11.6\n\nUsing bench::press() I’ll iterate over a vector of columns to evaluate performance speed as ncol increases and then visualize the execution time for data frames and lists.\n\n\nresults &lt;- bench::press(\n  ncol = c(1,10,35,75,100,300,500,650,800),\n  bench.means(ncol)\n)\n\n# Customize fonts\nfont_add_google(\"Libre Franklin\",\"libre\")\nshowtext_auto()\ntext &lt;- \"libre\"\n\n# Custom color palette\ncols &lt;- c(\"#80ADBF\",\"#7c225c\")\n\n# Create custom plot\nggplot(results,\n       aes(ncol,median,col = attr(expression,\"description\"))) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(x = \"Number of columns\",\n       y = \"Execution Time (ms)\",\n       colour = \"Data Structure\") +\n  scale_color_manual(values = cols) +\n  theme_minimal() +\n  theme(legend.position = \"top\",\n        legend.title = element_text(family = text, size = 23),\n        legend.text = element_text(family = text, size = 18),\n        plot.background = element_rect(fill = \"#dfe8f3\",\n                                       color = \"NA\"),\n        axis.title = element_text(family = text, size = 18,\n                                  hjust = .5),\n        axis.text = element_text(family = text,size = 16),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()\n        )\n\n\n\n\n\n\n\n\nx &lt;- make.rand(nrow = 1e4,ncol = 5)\nmeans &lt;- vapply(x,mean,numeric(1))\n\nfor(i in seq_along(means)){\n  x[[i]] &lt;- x[[i]] + means[[i]]\n}\n\n# How many times is the data frame copied during one iteration?\ncat(tracemem(x), \"\\n\")\n# &lt;0x000001b594610f08&gt;\n\nfor(i in 1:2){\n  x[[i]] &lt;- x[[i]] + means[[i]]\n}\n\n# tracemem[0x000001b594610f08 -&gt; 0x000001b597fb9468]\n# tracemem[0x000001b597fb9468 -&gt; 0x000001b597fb93f8]: [[&lt;-.data.frame [[&lt;-\n# tracemem[0x000001b597fb93f8 -&gt; 0x000001b597fb9388]\n# tracemem[0x000001b597fb9388 -&gt; 0x000001b597fb9318]: [[&lt;-.data.frame [[&lt;-\n\nuntracemem(x)\n\nThe execution time for a data frame is significantly higher after a few hundred columns are added. R’s copy-on modify behavior creates three copies of the data frame with each iteration of the for loop while a list is only copied once. This behavior is not problematic for a few columns so this approach may not pay off."
  },
  {
    "objectID": "learn/advanceR/advanceR/chap1_3/index.html#vectors",
    "href": "learn/advanceR/advanceR/chap1_3/index.html#vectors",
    "title": "Chapters 1–3",
    "section": "Vectors",
    "text": "Vectors\nVectors come in two flavors, atomic6 and lists. There are four primary types of atomic vectors: logical, integer, double, and character. We refer to integer and double vectors as numeric. There are also complex and raw vectors to deal with complex numbers and binary data7.\n\n\n\n\n\nflowchart LR\n  A[Logical] ---&gt; B((Atomic))\n  B --&gt; C{Vector}\n  E(Numeric) --&gt; B\n  D[Character] ---&gt; B\n  F[Double] --&gt; E\n  G[Integer] --&gt; E\n\n\n\n\n\n\nTo create scalars8 of each type of atomic vectors we use TRUE or FALSE9 for logicals. Doubles are specified in decimal, scientific or hexadecimal form and can take three special forms, Inf, -Inf, and NaN. Integers are written like doubles but must be followed by L, while strings are surrounded by \" or '.\n\ndbl &lt;- c(1.2,2.45,3.0)\nint &lt;- c(1,6,10,4)\nlog &lt;- c(TRUE,FALSE,TRUE,FALSE)\nchar &lt;- c(\"TRUE\",\"FALSE\",\"TRUE\",\"FALSE\")\n\ntypeof(dbl); length(dbl)\n#| [1] \"double\"\n#| [1] 3\ntypeof(int); length(int)\n#| [1] \"double\"\n#| [1] 4\ntypeof(log); length(log)\n#| [1] \"logical\"\n#| [1] 4\ntypeof(char); length(char)\n#| [1] \"character\"\n#| [1] 4\n\n# NA values\n# Most computations involving a missing value will return a missing value\nNA * 5\n#| [1] NA\nNA &gt; 10\n#| [1] NA\n!NA\n#| [1] NA\nNA^0\n#| [1] 1\nNA | TRUE # TRUE has a value of 1\n#| [1] TRUE\nNA & FALSE # FALSE has a value of 0\n#| [1] FALSE\n\n# Missing values propagate\nx &lt;- c(NA,5,10,13,NA)\nx == NA\n#| [1] NA NA NA NA NA\n#Use is.na() instead\nis.na(x)\n#| [1]  TRUE FALSE FALSE FALSE  TRUE\n\nInstead of testing whether you’re working with a vector using is.vector() or is.atomic(), or is.numeric()10; use more specific commands like is.logical(), is.double(), is.integer(),is.character(). For atomic vectors, type is a property of the entire vector– all elements must be of the same type. This will be forcibly achieved by automatic coercion in a pre-determined order:\n\n\n\n\n\nflowchart LR\nA(character) --&gt; B(double) \nB --&gt; C(integer) \nC --&gt; D(logical)\n\n\n\n\n\n\nYou can deliberately coerce an object by using as.*(); however, you run the risk of producing NA if the object cannot be coerced, i.e, coercing character strings to numeric objects.\nAttributes\nVectors do not include matrices, arrays,factors, or date–time objects. These types are built as attributes of a vector. These attributes are name–value pairs that attach metadata to an object. You can set attributes with structure(), and retrieved with attr() and attributes(), individually or collectively. However, most attributes, except names and dim, are lost by most operations, unless you create an S3 class for them.\n\na &lt;- 1:3\na &lt;- structure(1:3,\n  x = \"ay\",\n  y = \"caramba\"\n)\n\nstr(attributes(a))\n#| List of 2\n#|  $ x: chr \"ay\"\n#|  $ y: chr \"caramba\"\n\n#Don't use attr(a,\"names\") to assign names to elements in an object.\n# Instead use one of these three options:\nb &lt;- c(d = c(\"1,2,3\"),e = \"ay\", f = \"caramba\") \n\n# or use names()#\nb &lt;- c(\"1,2,3\",\"ay\",\"caramba\")\nnames(b) &lt;- c(\"d\",\"e\",\"f\")\n\n# or Inline, with setNames #\nb &lt;- setNames(c(\"1,2,3\",\"ay\",\"caramba\"), c(\"d\",\"e\",\"f\"))\n\nThe dim attribute allows a vector to behave like a two–dimensional matrix or a multi–dimensional array. Matrices and arrays can therefore be created in place by using the dim() command or matrix() and array()\n\n\n\nCommand Generalizations\n\nVector\nMatrix\nArray\n\n\n\nnames()\nrownames(), colnames()\ndimnames()\n\n\nlength()\nnrow(), ncol()\ndim()\n\n\nc()\nrbind(),cbind()\nabind::abind()\n\n\n—\nt()\naperm()\n\n\nis.null(dim(x))\nis.matrix()\nis.array()\n\n\n\n\n\nS3 Vectors\nThe class attribute turns vectors into S3 objects. S3 objects are built on top of a base type, and store additional information in other attributes. Four important S3 vectors used in R are factors, categorical data with values represening a fixed set of levels;date vectors, date—times stored in POSIXct, and difftime to represent durations.\n\n\n\n\n\nflowchart LR\n  A[Logical] ---&gt; B((Atomic))\n  B --&gt; C{Vector}\n  E(Numeric) --&gt; B\n  D[Character] ---&gt; B\n  F[Double] --&gt; E\n  G[Integer] --&gt; E\n  H[/factor/] --&gt;G\n  I[/POSIXct/] --&gt;F\n  J[/Date/] --&gt; F\n\n\n\n\n\n\nFactors\nFactors are built on top of an integer vector with two attributes: a class “factor” and levelsto define the set of allowed values. Tabulating factor vectors will get you a count of all categories, observed and unobserved, which is not the case with character vectors. Ordered factors are a variation of factors where the order of levels is meaningful(poor, good, best).\n\nf &lt;- factor(c(\"ay\", \"ay\",\"caramba\",\"caramba\",\"123\"))\nprint(f)\n#| [1] ay      ay      caramba caramba 123    \n#| Levels: 123 ay caramba\n\ntypeof(f)\n#| [1] \"integer\"\n\n# Factor levels count\ngender &lt;- c(\"m\",\"m\",\"m\")\ngender_factor &lt;- factor(gender, levels = c(\"m\",\"f\"))\n\n# Tabulate\ntable(gender)\n#| gender\n#| m \n#| 3\ntable(gender_factor)\n#| gender_factor\n#| m f \n#| 3 0\n\n# Ordered Factors\nhealth &lt;- ordered(c(\"poor\",\"good\",\"poor\",\"best\",\"good\"), \n                  levels = c(\"poor\",\"good\",\"best\"))\nprint(health)\n#| [1] poor good poor best good\n#| Levels: poor &lt; good &lt; best\n\nDates & Date—times\nDate vectors are built on top of double vectors and only have one attribute, class. The value of the double vector, after stripping the class, represents the number of days since the Unix epoch(1970-01-01).\nDate—times vector come in two formats, POSIXct and POSIXlt, built on top of double vectors, where the values of POSIXct, calendar time, represents the number of seconds since the Unix epoch.\n\ntoday &lt;- Sys.Date()\n\ntypeof(today)\n#| [1] \"double\"\nattributes(today)\n#| $class\n#| [1] \"Date\"\n\ndate &lt;- as.Date(\"1970-02-02\")\nunclass(date)\n#| [1] 32\n\n# Date-times\npoint_ct &lt;- as.POSIXct(\"2020-02-02\", tz = \"UTC\") # tz is timezone\nprint(point_ct)\n#| [1] \"2020-02-02 UTC\"\ntypeof(point_ct)\n#| [1] \"double\"\nattributes(point_ct)\n#| $class\n#| [1] \"POSIXct\" \"POSIXt\" \n#| \n#| $tzone\n#| [1] \"UTC\"\n\nDurations\nDurations represent the amount of time between pairs of dates or date—times stored as “difftimes”. Difftimes are also built on top of doubles and have a units attribute that determines how the integer is interpreted.\n\nweek &lt;- as.difftime(1,units = \"weeks\")\nprint(week)\n#| Time difference of 1 weeks\ntypeof(week)\n#| [1] \"double\"\nattributes(week)\n#| $class\n#| [1] \"difftime\"\n#| \n#| $units\n#| [1] \"weeks\"\n\nweek_days &lt;- as.difftime(7,units = \"days\")\nprint(week_days)\n#| Time difference of 7 days\ntypeof(week_days)\n#| [1] \"double\"\nattributes(week_days)\n#| $class\n#| [1] \"difftime\"\n#| \n#| $units\n#| [1] \"days\"\n\nLists\nLists are one level above atomic vectors and can contain elements of any type, not just vectors. Lists are constructed with the list() command. Because elements in a list are really just references to objects, creating a list doesn’t involve copying the objects into the list. In general, lists can contain any vector, matrix, data frame or array. Lists are also recursive because they can contain other lists, which makes them fundamentally different from vectors.\n\n#lists can contain any objects(vector, df, matrix)\nl1 &lt;- list(c(1:6, mtcars,matrix(1:20,4,5)))\ntypeof(l1)\n#| [1] \"list\"\n# Lists are smaller than expected\nlobstr::obj_size(l1)\n#| 6.33 kB\n\n# Recursive property\nl2 &lt;- list(list(list(1:200)))\nlobstr::obj_size(l2)\n#| 848 B\nstr(l2)\n#| List of 1\n#|  $ :List of 1\n#|   ..$ :List of 1\n#|   .. ..$ : int [1:200] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Turn list into vector…and face the consequences\n#| unlist(l1)\n\nMatrices & Arrays\nMatrices and arrays are created by using the dimension attribute of atomic vectors. We can also create list–matrices and list–arrays by means of the dim attribute. These structures can be helpful for modeling objects on a spatio–temporal 3D grid.\n\nl3 &lt;- list(1:6, a:f,1.25, c(T,F,T,F,F))\ndim(l3) &lt;- c(2,2)\nprint(l3)\n#|      [,1]      [,2]     \n#| [1,] integer,6 1.25     \n#| [2,] integer,2 logical,5\n\n# select 1 col, row 1\nl3[[1,1]]\n#| [1] 1 2 3 4 5 6\n\nData frames & Tibbles\nA data frame is a named list of vectors with attributes for names of columns and row.names, and its class data.frame. Data frames must be made up of vectors of equal length since by definition, they must be rectangular structures.\n\ndf1 &lt;- data.frame(x = 1:6, y = letters[1:6])\ntypeof(df1)\n#| [1] \"list\"\n\nattributes(df1)\n#| $names\n#| [1] \"x\" \"y\"\n#| \n#| $class\n#| [1] \"data.frame\"\n#| \n#| $row.names\n#| [1] 1 2 3 4 5 6\n\n# no. of columns\nlength(df1)\n#| [1] 2\n\n# Column names\nnames(df1)\n#| [1] \"x\" \"y\"\n\nTibbles are provided by the tibble package. They’re very similar to data frames, except that the class vector is longer, and they don’t coerce their input.\n\ndf2 &lt;- tibble::tibble(x = 1:6, y = letters[1:6])\ntypeof(df2)\n#| [1] \"list\"\n\nattributes(df2)\n#| $class\n#| [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n#| \n#| $row.names\n#| [1] 1 2 3 4 5 6\n#| \n#| $names\n#| [1] \"x\" \"y\"\n\nlength(df2)\n#| [1] 2\n\nnames(df2)\n#| [1] \"x\" \"y\"\n\nBoth, data frames and tibbles, do recycle shorter inputs; however, recycling only occur for columns that are integer multiples of the longest column and for vectors of length one, respectively. Finally, tibbles do allow you to call other variables during construction.\n\ndata.frame(x = 1:6, y = 1:3)\n#| x y\n#| 1 1 1\n#| 2 2 2\n#| 3 3 3\n#| 4 4 1\n#| 5 5 2\n#| 6 6 3\ndata.frame(x = 1:7, y = 1:3)\n#| Error in `data.frame()`:! arguments imply differing number of rows: 7, 3\n\ntibble::tibble(x = 1:4, y = 1)\n#| # A tibble: 4 × 2\n#|     x     y\n#| 1     1     1\n#| 2     2     1\n#| 3     3     1\n#| 4     4     1\ntibble::tibble(x = 1:4, y = 1:2)\n#| Error in `tibble::tibble()`: ! Tibble columns must have compatible sizes.\n#| • Size 4: Existing data.\n#| • Size 2: Column `y`.\n#| ℹ Only values of size one are recycled.\n#| \ndata.frame(x = 1:4, y = x * 2)\n#| Error: object 'x' not found\n#| \ntibble::tibble(x = 1:4, y = x * 2)\n# A tibble: 4 × 2\n#|      x     y\n#|  &lt;int&gt; &lt;dbl&gt;\n#| 1     1     2\n#| 2     2     4\n#| 3     3     6\n#| 4     4     8\n\nYou can also create data frames and tibbles with list columns by either adding the list/matrix after creating the data frame or by wrapping it in I()11. Matrices, on the other hand, must have the same number of rows as the data frame.\n\n# Add after creation\ndf &lt;- data.frame(x = 1:3)\ndf$y &lt;- list(1:2, 1:3,1:4)\n\n# Use I()\ndata.frame(x = 1:3, \n          y = I(list(1:2, 1:3,1:4)))\n#|   x          y\n#| 1 1       1, 2\n#| 2 2    1, 2, 3\n#| 3 3 1, 2, 3, 4\n\n# Tibbles can have lists direclty included\ntibble::tibble(x = 1:3, \n               y = list(1:2, 1:3, 1:4))\n#| # A tibble: 3 × 2\n#|       x y        \n#|   &lt;int&gt; &lt;list&gt;   \n#| 1     1 &lt;int [2]&gt;\n#| 2     2 &lt;int [3]&gt;\n#| 3     3 &lt;int [4]&gt;\n\n# Matrices are added after creation\ndfmat &lt;- data.frame(x = 1:3 * 5)\ndfmat$y &lt;- matrix(1:15, nrow = 3) # same no. of rows as dfmat\n\nstr(dfmat)\n#| 'data.frame':    3 obs. of  2 variables:\n#|  $ x: num  5 10 15\n#|  $ y: int [1:3, 1:5] 1 2 3 4 5 6 7 8 9 10 ..."
  },
  {
    "objectID": "learn/advanceR/advanceR/chap1_3/index.html#footnotes",
    "href": "learn/advanceR/advanceR/chap1_3/index.html#footnotes",
    "title": "Chapters 1–3",
    "section": "Footnotes",
    "text": "Footnotes\n\nCheck out the complete list of restricted names in ?Reserved↩︎\nALTREP, or alternative representation, allows R to represent certain vectors in a very compact fashion. Number sequences are one example of this behavior since only the first and last numbers are stored. For this reason every sequence is the same size, 680B.↩︎\nIf an object has a single name bound to it, R will modify–in–place–the new value will be appended to the same object.↩︎\nWhen an object is not longer bound to a name, the garbage collector will delete it to free up memory↩︎\neach iteration of the loop copies the data frame three times. Ideally, this behavior is reduced by using lists↩︎\nwhen the inputs of a vector are atomic, the resulting vector will also be atomic. This is called flattening.↩︎\nReview the documentation for as.raw() and as.complex().↩︎\na special syntax to create an individual value↩︎\nalso abbreviated as T and F↩︎\nThese commands don’t test if you have a vector. Review the documentation to figure out what they do.↩︎\nReview the documentation for AsIs()↩︎"
  },
  {
    "objectID": "learn/epi/index.html#epi-by-design",
    "href": "learn/epi/index.html#epi-by-design",
    "title": "Epi & Stats",
    "section": "Epi by Design",
    "text": "Epi by Design\n\n\n    \n    \n                  \n            Oct, 24\n        \n        \n            Deault Title Here\n\n            \n\n            Subtitle Here\n            \n            \n        \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "learn/advanceR/index.html#advanced-r",
    "href": "learn/advanceR/index.html#advanced-r",
    "title": "Programming",
    "section": "Advanced R",
    "text": "Advanced R\n\n\n    \n    \n                  \n            Oct, 24\n        \n        \n            Chapters 1–2\n\n            \n\n            \n            \n            \n        \n        \n        \n        \n            \n                Thought you were normal\n            \n                 / but Bayes says you're different;\n            \n                 / heart's torn, you or him.\n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "learn/advanceR/index.html#statistical-rethinking",
    "href": "learn/advanceR/index.html#statistical-rethinking",
    "title": "Programming",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\n\n    \n    \n                  \n            Oct, 24\n        \n        \n            Chapters 1–2\n\n            \n\n            \n            \n            \n        \n        \n        \n        \n            \n                Thought you were normal\n            \n                 / but Bayes says you're different;\n            \n                 / heart's torn, you or him.\n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "autotelic",
    "section": "",
    "text": "A site dedicated to my programming and statistics learning journey where I’ll record what I read, summarize, explain and learn from a wide variety of topics all related to programming, statistics, and occasionally, some other subjects.\nThe goal is that, within a 12-month period, I’ll be able to learn from three good, solid books and use this new found knowledge in my day–to–day life to create and improve something, however small it is. At the end of the year, I should see the tangible fruit of my labors, either virtually on the “interweb” or in physical form."
  },
  {
    "objectID": "learn/art/index.html#the-art-of-calligraphy",
    "href": "learn/art/index.html#the-art-of-calligraphy",
    "title": "Arts & Thangs",
    "section": "The Art of Calligraphy",
    "text": "The Art of Calligraphy\n\n\n    \n    \n                  \n            Oct, 24\n        \n        \n            Deault Title Here\n\n            \n\n            Subtitle Here\n            \n            \n        \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "learn/advanceR/Stat_Rethinking/ch1_3/index.html",
    "href": "learn/advanceR/Stat_Rethinking/ch1_3/index.html",
    "title": "Chapters 1–2",
    "section": "",
    "text": "The Golem of Prague - Jewish mythical creature that is awakened and commanded by truth but lacks wisdom. Its lack of discernment renders him dangerous and vulnerable to the work of evil agents.\nStatistical models are like golems, set up and commanded by truth to obey without complaints but unable to discern context and usefulness.\nA clear and urgent need for a unified theory of statistics that allows for flexibility and freedom in designing, building and refining special-purpose statistical procedures. Classic statistics tests are rigid and applicable to only a handful of procedures; but even those tests that offer more flexibility, like ordinary linear regression relies on strong assumptions that lead to catastrophic results if violated.\n\n\nDF is impossible in nearly every scientific context:\n\nNHST is not falsificationist since it doesn’t falsify the research hypothesis, but the null of no effect.\nHypothesis are not models. One hypothesis may be represented by many different models, and nay given statistical model may correspond to more than one hypothesis.\nPossible solution: Compare more than one model. All statistical tests are also models.\n\n\n\n\nBayesian data analysis\n\ncount the number of ways things can happen, according to your assumptions.\nRandomness is a property of information, not the world.\n\n\nMultilevel models\n\nThe Earth stands on the back of an elephant, which stands on the back of a turtle. Where does the turtle stand? “It’s turtles all the way down”…for us it’s parameters all the way down.\nMultilevel regression deserves to be the default form of regression\nTo adjust estimates for repeat sampling\nTo adjust estimates for imbalance in sampling\nTo study variation\nTo avoid averaging\n\n\nModel comparison using uniform criteria\n\nModel comparison based on future predictive accuracy\n\n\n\n\n\nChapter 2 and 3 - foundation Bayesian inference tools\n\nChapter 4 - 7 - build multiple linear regression as Bayesian tools.\n\nChapter 8 - 11 - generalized linear models:\n\nMarkov Chain Monte Carlo (MCMC)\nMaximum Entropy\nModel details\n\n\n\nChapter 12 - 14 - Multilevel models, linear and generalized, missing data, measurement error, and spatial correlation.\n\nChapter 15 - returns to some of the issues raised in Chapter 1."
  },
  {
    "objectID": "learn/advanceR/Stat_Rethinking/ch1_3/index.html#chapter-1",
    "href": "learn/advanceR/Stat_Rethinking/ch1_3/index.html#chapter-1",
    "title": "Chapters 1–2",
    "section": "",
    "text": "The Golem of Prague - Jewish mythical creature that is awakened and commanded by truth but lacks wisdom. Its lack of discernment renders him dangerous and vulnerable to the work of evil agents.\nStatistical models are like golems, set up and commanded by truth to obey without complaints but unable to discern context and usefulness.\nA clear and urgent need for a unified theory of statistics that allows for flexibility and freedom in designing, building and refining special-purpose statistical procedures. Classic statistics tests are rigid and applicable to only a handful of procedures; but even those tests that offer more flexibility, like ordinary linear regression relies on strong assumptions that lead to catastrophic results if violated.\n\n\nDF is impossible in nearly every scientific context:\n\nNHST is not falsificationist since it doesn’t falsify the research hypothesis, but the null of no effect.\nHypothesis are not models. One hypothesis may be represented by many different models, and nay given statistical model may correspond to more than one hypothesis.\nPossible solution: Compare more than one model. All statistical tests are also models.\n\n\n\n\nBayesian data analysis\n\ncount the number of ways things can happen, according to your assumptions.\nRandomness is a property of information, not the world.\n\n\nMultilevel models\n\nThe Earth stands on the back of an elephant, which stands on the back of a turtle. Where does the turtle stand? “It’s turtles all the way down”…for us it’s parameters all the way down.\nMultilevel regression deserves to be the default form of regression\nTo adjust estimates for repeat sampling\nTo adjust estimates for imbalance in sampling\nTo study variation\nTo avoid averaging\n\n\nModel comparison using uniform criteria\n\nModel comparison based on future predictive accuracy\n\n\n\n\n\nChapter 2 and 3 - foundation Bayesian inference tools\n\nChapter 4 - 7 - build multiple linear regression as Bayesian tools.\n\nChapter 8 - 11 - generalized linear models:\n\nMarkov Chain Monte Carlo (MCMC)\nMaximum Entropy\nModel details\n\n\n\nChapter 12 - 14 - Multilevel models, linear and generalized, missing data, measurement error, and spatial correlation.\n\nChapter 15 - returns to some of the issues raised in Chapter 1."
  },
  {
    "objectID": "learn/advanceR/Stat_Rethinking/ch1_3/index.html#chapter-2---small-worlds-and-large-worlds",
    "href": "learn/advanceR/Stat_Rethinking/ch1_3/index.html#chapter-2---small-worlds-and-large-worlds",
    "title": "Chapters 1–2",
    "section": "Chapter 2 - Small Worlds and Large Worlds",
    "text": "Chapter 2 - Small Worlds and Large Worlds\nChristopher Colombo miscalculated the diameter of the earth– he thought it was smaller (30,000 km instead of 40,000), landed in the Bahamas and discover a new world.\n\nThe model is the small world, and we hope to deploy it in the large world of reality and be useful.\nBayesian models are optimal in small worlds, but need to be demonstrated rather than logically deduced in large worlds.\n\nThe garden of forking data\n\nConsider everything that could have happened.\n\n\nCount the possibilities\nUse prior information\n\nfrom previous data\nknowledge of how the process works\nAct as if you had prior information\n\nmultiply the prior count by the new count\nWhen we have previous information suggesting there are \\(W_{prior}\\) ways for a conjecture to produce a previous observation \\(D_{prior}\\) and,\nWe acquire new observations \\(D_{new}\\) then the same conjecture can produce in \\(W_{new}\\) ways\nthe number of ways the conjecture can account for \\(D_{prior}\\) and \\(D_{new}\\) is the product of \\(W_{prior} \\times W_{new}\\).\n\n\nfrom count to probability \\[\n\\begin{split}\n\\text{Plausibility of} X \\text{ after observing } x \\\\\n\\propto \\text{ ways } X \\text{ can produce } x \\\\\n\\times \\text{ prior plausibility of } X\n\\end{split}\n\\]\n\nstandardize the probabilities (divide by the sum of products)\n\n\n\n\n\nA few things to define:\n\nthe parameter is the conjectured proportion of blue marbles.\nthe likelihood is the relative number of ways that a p value can produce the data\nthe prior probability is the prior plausibility of any specific p values\nthe posterior probability is the new, updated plausibility of any specific p\n\nBuilding a model\nWorking with a toy model to get an idea of how Bayesian inference works\n\nSuppose you have a globe representing our planet, the Earth. This version of the world is small enough to hold in your hands. You are curious how much of the surface is covered in water. You adopt the following strategy: You will toss the globe up in the air. When you catch it, you will record whether or not the surface under your right index finger is water or land. Then you toss the globe up in the air again and repeat the procedure\n\nbut first, there are assumptions that constitute the model:\n\nData story: motivate the model by narrating how the data might arise\n\nHow the data came to be\n\nDescriptive story\nCausal story _ Involves restating:\n\nthe true proportion of the conjecture, p\n\nthe probability of producing an alternative conjecture, \\(1-p\\)\n\nindependence of conjectures\n\n\n\n\n\n\nUpdate: Educate your model by feeding it the data\n\nBegins with prior plausibilities\nUpdates them in light of the data\nA model can be updated forward-in-time, backwards or all-at-once. It can mathematically divide out the observations to infer the previous plausibility curve.\nA bad prior leads to misleading results just like bad estimators in Fisherian inference lead to bad results.\n\n\nEvaluate: All statistical models require supervision, leading possibly to model revision.\n\nReal world data must be accurately described to the model.\nModels are never true to reality so there’s no point in checking the truth of the model.\nYou either fail or succeed at recognizing the falseness of the model.\nCheck adequacy of the model, not its truth.\n\n\n\nLikelihood: the probability of any possible observation, for any possible state of the small world.\n\\[\nP(w|n,p) = \\frac{n!}{w!(n - w)!}p^w(1-p)^{(n-w)}\n\\]\n\n# Only two outcomes, W and L so p = 0.5, n = 9\nw &lt;- dbinom(6,size = 9, prob = 0.5)\nprint(w)\n#&gt; [1] 0.164\n\nParameters: the adjustable inputs (\\(n,p,w\\))\nPriors: for every parameter, there must be a prior probability and constrain the parameters to reasonable ranges.\nPosteriors: unique set of estimates for every combination of data, likelihood, parameters and priors that takes the form of the probability of the parameters, conditional on the data \\(P(p|n,w)\\), defined by Bayes theorem\n\\[\nP(w,p) = P(w|n,p)P(p)\nP(w,p) = P(p|w,n)P(w)\n\\] Bayes’ theorem is just the posterior probability of p given w. The product of the likelihood and prior, divided by \\(P(w)\\), the average likelihood over the prior1. \\[\nP(p|w,n) = \\frac{P(w|p)P(p)}{P(w)}\n\\]\nMaking the model go\nThe core of the model, its motor, conditions the prior on the data, which can be done without forcing simple and rigid forms of prior that are easy to work with in three of many ways:\n\nGrid approximation\n\ncontinuous parameters\nonly useful as a teaching tool\nScales very poorly\ndefine the grid: how many points to use in estimating the posterior, then make a list of parameters in the grid.\ncompute the value of the prior at each parameter value on the grid\ncompute the unstandardized posterior at each parameter value\nstandardize the posterior\n\n\n\n\n# Setting global parameter for base plots\npar(bg = \"#Fffff2\")\n\n# Define the grid\np_grid &lt;- seq(0,1,length.out = 20)\n\n# define the prior\nprior &lt;- rep(1,20)\n\n# compute the likelihood @ each value on the grid\nlikelihood &lt;- dbinom(6,9,prob = p_grid)\n\n# compute product of likelihood and prior\nunstd_posterior &lt;- likelihood * prior\n\n# standardize the posterior\nposterior &lt;- unstd_posterior/sum(unstd_posterior)\n\nplot(p_grid,posterior, type = \"b\",\n     bty= \"n\",\n     pch = 16,\n     col = \"#7c225c\",\n     family = font_family,\n     xlab = \"probability of water\",\n     ylab = \"posterior probability\")\nmtext(\"20 points\", family = font_family)\n\n\n\n\n\n\n\n# You can try sparser (&lt;20) or denser grids (&gt;100). The correct density \n# for your grid depends on how accurate you want your approximation to be.\n\n\nQuadratic approximation\n\nCalled quadratic approximation because the log of a normal distribution forms a parabola\nthe posterior distribution can be approximated by a Gaussian, under general conditions\nWe can make use of only two parameters, its center and spread\nFind the peak of the posterior (its center), then estimate the curvature near the peak\ncompute quadratic approximation using this estimate\nas n increases, the quadratic approximation gets better\nequivalent to maximum likelihood estimate (MLE) and its standard error\n\n\n\n\n# Setting global parameter for base plots\npar(bg = \"#Fffff2\")\n\nlibrary(rethinking)\n\nglobe_quad_approx &lt;- map(\n  \n  alist(\n    \n    w ~ dbinom(9,p), # binomial likelihood\n    p ~ dunif(0,1) # uniform prior\n  ),\n  data = list(w = 6)\n)\n\n# display summary of quad. approximation\n\nprecis(globe_quad_approx)\n#&gt;    mean    sd  5.5% 94.5%\n#&gt; p 0.667 0.157 0.416 0.918\n\n# Compare the quad. approximation to the analytical solution\n\nw &lt;- 6\nn &lt;- 9\n\n# exact posterior\ncurve(\n  dbeta(x,\n            w+1,\n            n - w+1),\n      0,1,\n  xlab = \"proportion water\",\n  ylab = \"density\",\n  bty = \"n\",\n  family = font_family)\n\n# quad. approximation\ncurve(dnorm(x,0.67,0.16), lty = 2, add = TRUE,\n      xlab = \"proportion water\",\n      ylab = \"density\",\n       col = \"#7c225c\",\n      bty = \"n\",\n      family = font_family)\naxis(1)\naxis(2)\n\n\n\n\n\n\n\n\nMCMC\n\nmodels for which grid or quadratic approximation are always satisfactory.\ndoesn’t compute or approximates the posterior but draws samples from the posterior.\nResults in a collection of parameters and its frequencies which correspond to the posterior plausibilities."
  },
  {
    "objectID": "learn/advanceR/Stat_Rethinking/ch1_3/index.html#practice-problems",
    "href": "learn/advanceR/Stat_Rethinking/ch1_3/index.html#practice-problems",
    "title": "Chapters 1–2",
    "section": "Practice Problems",
    "text": "Practice Problems\n2M1\n\nCode## 2M1(i,ii,iii)\n\n# Create function to compute prior, likelihood, posterior and print posterior plot\ncreate_posterior_plot &lt;- function(trials, \n                                  successes,\n                                  grid_points,\n                                  ...,\n                                  mtext_label = NULL,\n                                  xlab = NULL, \n                                  ylab = NULL){\n  \n  args &lt;- list(...)\n  \n  # Define the grid\n  p_grid &lt;- seq(0,1, length.out = grid_points)\n  \n  # Define the prior\n  prior &lt;- if (\"prior\" %in% names(args)) args$prior else rep(1, grid_points)\n  \n  # Ensure the prior length matches grid_points\n  if (length(prior) != grid_points) {\n    stop(\"Length of prior must match grid_points\")\n  }\n  \n  # Compute the likelihood for each value in the grid\n  likelihood &lt;- dbinom(successes,trials,prob = p_grid)\n  \n  # Compute unstandardized posterior\n  unstd_posterior &lt;- likelihood * prior\n  \n  # Standardize posterior\n  posterior &lt;- unstd_posterior/sum(unstd_posterior)\n  \n  plot(p_grid, posterior, type = \"b\",\n       bty = \"n\",\n         xlab = xlab,\n         ylab = ylab,\n       pch = 16, \n       col = \"#7c225c\",\n       family = font_family)\n    mtext(mtext_label, \n          family = font_family)\n    \n    # Placement of x-axis\n    axis(1, lwd = 0.5)\n    # Placement of y-axis\n    axis(2, lwd = 0.5)\n}\n\npar(mfrow = c(1,3), \n    bg = \"#Fffff2\")\n\ncreate_posterior_plot(3,3,20,\n                      mtext_label = \"100 %success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(4,3,20,\n                      mtext_label = \"75% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(7,5,20,\n                      mtext_label = \"71% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\n\n\n\n\n\n\n2M2\n\nCode# Setting global parameter for base plots\npar(mfrow = c(1,3), bg = \"#Fffff2\")\n\n# 2M2 - assume a prior equal to zero when p &lt; 0.5 \n# and a positive constant when p &gt;= 0.5\np_grid &lt;- seq(0,1,length.out = 20)\n\ncreate_posterior_plot(3,3,\n                      grid_points = 20,\n                      prior = ifelse(p_grid &lt; 0.5,0,1),\n                      mtext_label = \"100% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(4,3,\n                      grid_points = 20,\n                      prior = ifelse(p_grid &lt; 0.5,0,1),\n                      mtext_label = \"75% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(7,5,\n                      grid_points = 20,\n                      prior = ifelse(p_grid &lt; 0.5,0,1),\n                      mtext_label = \"71% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\n\n\n\n\n\n\n2M3\nShow that the posterior probability of tossing the Earth given a land observation is \\(P(E|L) = .23\\) if the probability of observing water on the Earth is \\(P(W|E) = .7\\) and on Mars \\(P(W|M) = 0\\). Assume \\(P(E)  =.5\\)\n\\[\n\\begin{split}\nL = Land, W = Water, E = Earth, M = Mars \\\\\n\\\\\nP(L|E) = 0.3 \\\\\nP(E) = 0.5 \\\\\nP(M) = 0.5 \\\\\nP(L|M) = 1 \\\\ \\\\\nP(E|L) = \\frac{P(L|E)P(E)}{P(L)}\\\\ \\\\\nP(E|L) = \\frac{.3\\times .5}{(0.3 \\times .5 + 1\\times .5)}\\\\ \\\\\nP(E|L) = .231\n\\end{split}\n\\]"
  },
  {
    "objectID": "learn/advanceR/Stat_Rethinking/ch1_3/index.html#footnotes",
    "href": "learn/advanceR/Stat_Rethinking/ch1_3/index.html#footnotes",
    "title": "Chapters 1–2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\\(P(w) = E(P(w|p)) = \\int P(w|p)P(p)\\partial p\\)↩︎"
  }
]