[
  {
    "objectID": "learn/art/index.html#the-art-of-calligraphy",
    "href": "learn/art/index.html#the-art-of-calligraphy",
    "title": "Arts & Thangs",
    "section": "The Art of Calligraphy",
    "text": "The Art of Calligraphy\n\n\n    \n    \n                  \n            Oct, 24\n        \n        \n            Deault Title Here\n\n            \n\n            Subtitle Here\n            \n            \n        \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "learn/Stat_Rethinking/index.html#statistical-rethinking",
    "href": "learn/Stat_Rethinking/index.html#statistical-rethinking",
    "title": "Programming",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\n\n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "autotelic",
    "section": "",
    "text": "A site dedicated to my programming and statistics learning journey where I’ll record what I read, summarize, explain and learn from a wide variety of topics all related to programming, statistics, and occasionally, some other subjects.\nThe goal is that, within a 12-month period, I’ll be able to learn from three good, solid books and use this new found knowledge in my day–to–day life to create and improve something, however small it is. At the end of the year, I should see the tangible fruit of my labors, either virtually on the “interweb” or in physical form."
  },
  {
    "objectID": "learn/advanceR/index.html#advanced-r",
    "href": "learn/advanceR/index.html#advanced-r",
    "title": "Programming",
    "section": "Advanced R",
    "text": "Advanced R\n\n\n    \n    \n                  \n            Oct, 24\n        \n        \n            Chapters 1–3\n\n            \n\n            \n            \n            \n        \n        \n        \n        \n            \n                What's in a name?\n            \n                 / That which we call a rose\n            \n                 / by any other name would smell as sweet.\n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "learn/advanceR/index.html#statistical-rethinking",
    "href": "learn/advanceR/index.html#statistical-rethinking",
    "title": "Programming",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking"
  },
  {
    "objectID": "learn/epi/index.html#epi-by-design",
    "href": "learn/epi/index.html#epi-by-design",
    "title": "Epi & Stats",
    "section": "Epi by Design",
    "text": "Epi by Design\n\n\n    \n    \n                  \n            Oct, 24\n        \n        \n            Deault Title Here\n\n            \n\n            Subtitle Here\n            \n            \n        \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "learn/Stat_Rethinking/ch1_3/index.html",
    "href": "learn/Stat_Rethinking/ch1_3/index.html",
    "title": "Chapters 1–2",
    "section": "",
    "text": "The Golem of Prague - Jewish mythical creature that is awakened and commanded by truth but lacks wisdom. Its lack of discernment renders him dangerous and vulnerable to the work of evil agents.\nStatistical models are like golems, set up and commanded by truth to obey without complaints but unable to discern context and usefulness.\nA clear and urgent need for a unified theory of statistics that allows for flexibility and freedom in designing, building and refining special-purpose statistical procedures. Classic statistics tests are rigid and applicable to only a handful of procedures; but even those tests that offer more flexibility, like ordinary linear regression relies on strong assumptions that lead to catastrophic results if violated.\n\n\nDF is impossible in nearly every scientific context:\n\nNHST is not falsificationist since it doesn’t falsify the research hypothesis, but the null of no effect.\nHypothesis are not models. One hypothesis may be represented by many different models, and nay given statistical model may correspond to more than one hypothesis.\nPossible solution: Compare more than one model. All statistical tests are also models.\n\n\n\n\nBayesian data analysis\n\ncount the number of ways things can happen, according to your assumptions.\nRandomness is a property of information, not the world.\n\n\nMultilevel models\n\nThe Earth stands on the back of an elephant, which stands on the back of a turtle. Where does the turtle stand? “It’s turtles all the way down”…for us it’s parameters all the way down.\nMultilevel regression deserves to be the default form of regression\nTo adjust estimates for repeat sampling\nTo adjust estimates for imbalance in sampling\nTo study variation\nTo avoid averaging\n\n\nModel comparison using uniform criteria\n\nModel comparison based on future predictive accuracy\n\n\n\n\n\nChapter 2 and 3 - foundation Bayesian inference tools\n\nChapter 4 - 7 - build multiple linear regression as Bayesian tools.\n\nChapter 8 - 11 - generalized linear models:\n\nMarkov Chain Monte Carlo (MCMC)\nMaximum Entropy\nModel details\n\n\n\nChapter 12 - 14 - Multilevel models, linear and generalized, missing data, measurement error, and spatial correlation.\n\nChapter 15 - returns to some of the issues raised in Chapter 1."
  },
  {
    "objectID": "learn/Stat_Rethinking/ch1_3/index.html#chapter-1",
    "href": "learn/Stat_Rethinking/ch1_3/index.html#chapter-1",
    "title": "Chapters 1–2",
    "section": "",
    "text": "The Golem of Prague - Jewish mythical creature that is awakened and commanded by truth but lacks wisdom. Its lack of discernment renders him dangerous and vulnerable to the work of evil agents.\nStatistical models are like golems, set up and commanded by truth to obey without complaints but unable to discern context and usefulness.\nA clear and urgent need for a unified theory of statistics that allows for flexibility and freedom in designing, building and refining special-purpose statistical procedures. Classic statistics tests are rigid and applicable to only a handful of procedures; but even those tests that offer more flexibility, like ordinary linear regression relies on strong assumptions that lead to catastrophic results if violated.\n\n\nDF is impossible in nearly every scientific context:\n\nNHST is not falsificationist since it doesn’t falsify the research hypothesis, but the null of no effect.\nHypothesis are not models. One hypothesis may be represented by many different models, and nay given statistical model may correspond to more than one hypothesis.\nPossible solution: Compare more than one model. All statistical tests are also models.\n\n\n\n\nBayesian data analysis\n\ncount the number of ways things can happen, according to your assumptions.\nRandomness is a property of information, not the world.\n\n\nMultilevel models\n\nThe Earth stands on the back of an elephant, which stands on the back of a turtle. Where does the turtle stand? “It’s turtles all the way down”…for us it’s parameters all the way down.\nMultilevel regression deserves to be the default form of regression\nTo adjust estimates for repeat sampling\nTo adjust estimates for imbalance in sampling\nTo study variation\nTo avoid averaging\n\n\nModel comparison using uniform criteria\n\nModel comparison based on future predictive accuracy\n\n\n\n\n\nChapter 2 and 3 - foundation Bayesian inference tools\n\nChapter 4 - 7 - build multiple linear regression as Bayesian tools.\n\nChapter 8 - 11 - generalized linear models:\n\nMarkov Chain Monte Carlo (MCMC)\nMaximum Entropy\nModel details\n\n\n\nChapter 12 - 14 - Multilevel models, linear and generalized, missing data, measurement error, and spatial correlation.\n\nChapter 15 - returns to some of the issues raised in Chapter 1."
  },
  {
    "objectID": "learn/Stat_Rethinking/ch1_3/index.html#chapter-2---small-worlds-and-large-worlds",
    "href": "learn/Stat_Rethinking/ch1_3/index.html#chapter-2---small-worlds-and-large-worlds",
    "title": "Chapters 1–2",
    "section": "Chapter 2 - Small Worlds and Large Worlds",
    "text": "Chapter 2 - Small Worlds and Large Worlds\nChristopher Colombo miscalculated the diameter of the earth– he thought it was smaller (30,000 km instead of 40,000), landed in the Bahamas and discover a new world.\n\nThe model is the small world, and we hope to deploy it in the large world of reality and be useful.\nBayesian models are optimal in small worlds, but need to be demonstrated rather than logically deduced in large worlds.\n\nThe garden of forking data\n\nConsider everything that could have happened.\n\n\nCount the possibilities\nUse prior information\n\nfrom previous data\nknowledge of how the process works\nAct as if you had prior information\n\nmultiply the prior count by the new count\nWhen we have previous information suggesting there are \\(W_{prior}\\) ways for a conjecture to produce a previous observation \\(D_{prior}\\) and,\nWe acquire new observations \\(D_{new}\\) then the same conjecture can produce in \\(W_{new}\\) ways\nthe number of ways the conjecture can account for \\(D_{prior}\\) and \\(D_{new}\\) is the product of \\(W_{prior} \\times W_{new}\\).\n\n\nfrom count to probability \\[\n\\begin{split}\n\\text{Plausibility of} X \\text{ after observing } x \\\\\n\\propto \\text{ ways } X \\text{ can produce } x \\\\\n\\times \\text{ prior plausibility of } X\n\\end{split}\n\\]\n\nstandardize the probabilities (divide by the sum of products)\n\n\n\n\n\nA few things to define:\n\nthe parameter is the conjectured proportion of blue marbles.\nthe likelihood is the relative number of ways that a p value can produce the data\nthe prior probability is the prior plausibility of any specific p values\nthe posterior probability is the new, updated plausibility of any specific p\n\nBuilding a model\nWorking with a toy model to get an idea of how Bayesian inference works\n\nSuppose you have a globe representing our planet, the Earth. This version of the world is small enough to hold in your hands. You are curious how much of the surface is covered in water. You adopt the following strategy: You will toss the globe up in the air. When you catch it, you will record whether or not the surface under your right index finger is water or land. Then you toss the globe up in the air again and repeat the procedure\n\nbut first, there are assumptions that constitute the model:\n\nData story: motivate the model by narrating how the data might arise\n\nHow the data came to be\n\nDescriptive story\nCausal story _ Involves restating:\n\nthe true proportion of the conjecture, p\n\nthe probability of producing an alternative conjecture, \\(1-p\\)\n\nindependence of conjectures\n\n\n\n\n\n\nUpdate: Educate your model by feeding it the data\n\nBegins with prior plausibilities\nUpdates them in light of the data\nA model can be updated forward-in-time, backwards or all-at-once. It can mathematically divide out the observations to infer the previous plausibility curve.\nA bad prior leads to misleading results just like bad estimators in Fisherian inference lead to bad results.\n\n\nEvaluate: All statistical models require supervision, leading possibly to model revision.\n\nReal world data must be accurately described to the model.\nModels are never true to reality so there’s no point in checking the truth of the model.\nYou either fail or succeed at recognizing the falseness of the model.\nCheck adequacy of the model, not its truth.\n\n\n\nLikelihood: the probability of any possible observation, for any possible state of the small world.\n\\[\nP(w|n,p) = \\frac{n!}{w!(n - w)!}p^w(1-p)^{(n-w)}\n\\]\n\n# Only two outcomes, W and L so p = 0.5, n = 9\nw &lt;- dbinom(6,size = 9, prob = 0.5)\nprint(w)\n#&gt; [1] 0.164\n\nParameters: the adjustable inputs (\\(n,p,w\\))\nPriors: for every parameter, there must be a prior probability and constrain the parameters to reasonable ranges.\nPosteriors: unique set of estimates for every combination of data, likelihood, parameters and priors that takes the form of the probability of the parameters, conditional on the data \\(P(p|n,w)\\), defined by Bayes theorem\n\\[\nP(w,p) = P(w|n,p)P(p)\nP(w,p) = P(p|w,n)P(w)\n\\] Bayes’ theorem is just the posterior probability of p given w. The product of the likelihood and prior, divided by \\(P(w)\\), the average likelihood over the prior1. \\[\nP(p|w,n) = \\frac{P(w|p)P(p)}{P(w)}\n\\]\nMaking the model go\nThe core of the model, its motor, conditions the prior on the data, which can be done without forcing simple and rigid forms of prior that are easy to work with in three of many ways:\n\nGrid approximation\n\ncontinuous parameters\nonly useful as a teaching tool\nScales very poorly\ndefine the grid: how many points to use in estimating the posterior, then make a list of parameters in the grid.\ncompute the value of the prior at each parameter value on the grid\ncompute the unstandardized posterior at each parameter value\nstandardize the posterior\n\n\n\n\n# Setting global parameter for base plots\npar(bg = \"#Fffff2\")\n\n# Define the grid\np_grid &lt;- seq(0,1,length.out = 20)\n\n# define the prior\nprior &lt;- rep(1,20)\n\n# compute the likelihood @ each value on the grid\nlikelihood &lt;- dbinom(6,9,prob = p_grid)\n\n# compute product of likelihood and prior\nunstd_posterior &lt;- likelihood * prior\n\n# standardize the posterior\nposterior &lt;- unstd_posterior/sum(unstd_posterior)\n\nplot(p_grid,posterior, type = \"b\",\n     bty= \"n\",\n     pch = 16,\n     col = \"#7c225c\",\n     family = font_family,\n     xlab = \"probability of water\",\n     ylab = \"posterior probability\")\nmtext(\"20 points\", family = font_family)\n\n\n\n\n\n\n\n# You can try sparser (&lt;20) or denser grids (&gt;100). The correct density \n# for your grid depends on how accurate you want your approximation to be.\n\n\nQuadratic approximation\n\nCalled quadratic approximation because the log of a normal distribution forms a parabola\nthe posterior distribution can be approximated by a Gaussian, under general conditions\nWe can make use of only two parameters, its center and spread\nFind the peak of the posterior (its center), then estimate the curvature near the peak\ncompute quadratic approximation using this estimate\nas n increases, the quadratic approximation gets better\nequivalent to maximum likelihood estimate (MLE) and its standard error\n\n\n\n\n# Setting global parameter for base plots\npar(bg = \"#Fffff2\")\n\nlibrary(rethinking)\n\nglobe_quad_approx &lt;- map(\n  \n  alist(\n    \n    w ~ dbinom(9,p), # binomial likelihood\n    p ~ dunif(0,1) # uniform prior\n  ),\n  data = list(w = 6)\n)\n\n# display summary of quad. approximation\n\nprecis(globe_quad_approx)\n#&gt;    mean    sd  5.5% 94.5%\n#&gt; p 0.667 0.157 0.416 0.918\n\n# Compare the quad. approximation to the analytical solution\n\nw &lt;- 6\nn &lt;- 9\n\n# exact posterior\ncurve(\n  dbeta(x,\n            w+1,\n            n - w+1),\n      0,1,\n  xlab = \"proportion water\",\n  ylab = \"density\",\n  bty = \"n\",\n  family = font_family)\n\n# quad. approximation\ncurve(dnorm(x,0.67,0.16), lty = 2, add = TRUE,\n      xlab = \"proportion water\",\n      ylab = \"density\",\n       col = \"#7c225c\",\n      bty = \"n\",\n      family = font_family)\naxis(1)\naxis(2)\n\n\n\n\n\n\n\n\nMCMC\n\nmodels for which grid or quadratic approximation are always satisfactory.\ndoesn’t compute or approximates the posterior but draws samples from the posterior.\nResults in a collection of parameters and its frequencies which correspond to the posterior plausibilities."
  },
  {
    "objectID": "learn/Stat_Rethinking/ch1_3/index.html#practice-problems",
    "href": "learn/Stat_Rethinking/ch1_3/index.html#practice-problems",
    "title": "Chapters 1–2",
    "section": "Practice Problems",
    "text": "Practice Problems\n2M1(i,ii,iii)\n\nCode## 2M1(i,ii,iii)\n\n# Create function to compute prior, likelihood, posterior and print posterior plot\ncreate_posterior_plot &lt;- function(trials, \n                                  successes,\n                                  grid_points,\n                                  ...,\n                                  mtext_label = NULL,\n                                  xlab = NULL, \n                                  ylab = NULL){\n  \n  args &lt;- list(...)\n  \n  # Define the grid\n  p_grid &lt;- seq(0,1, length.out = grid_points)\n  \n  # Define the prior\n  prior &lt;- if (\"prior\" %in% names(args)) args$prior else rep(1, grid_points)\n  \n  # Ensure the prior length matches grid_points\n  if (length(prior) != grid_points) {\n    stop(\"Length of prior must match grid_points\")\n  }\n  \n  # Compute the likelihood for each value in the grid\n  likelihood &lt;- dbinom(successes,trials,prob = p_grid)\n  \n  # Compute unstandardized posterior\n  unstd_posterior &lt;- likelihood * prior\n  \n  # Standardize posterior\n  posterior &lt;- unstd_posterior/sum(unstd_posterior)\n  \n  plot(p_grid, posterior, type = \"b\",\n       bty = \"n\",\n         xlab = xlab,\n         ylab = ylab,\n       pch = 16, \n       col = \"#7c225c\",\n       family = font_family)\n    mtext(mtext_label, \n          family = font_family)\n    \n    # Placement of x-axis\n    axis(1, lwd = 0.5)\n    # Placement of y-axis\n    axis(2, lwd = 0.5)\n}\n\npar(mfrow = c(1,3), \n    bg = \"#Fffff2\")\n\ncreate_posterior_plot(3,3,20,\n                      mtext_label = \"100 %success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(4,3,20,\n                      mtext_label = \"75% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\ncreate_posterior_plot(7,5,20,\n                      mtext_label = \"71% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")\n\n\n\n\n\n\n\n2M2\n\nCode# Setting global parameter for base plots\npar(bg = \"#Fffff2\")\n\n# 2M2 - assume a prior equal to zero when p &lt; 0.5 \n# and a positive constant when p &gt;= 0.5\np_grid &lt;- seq(0,1,length.out = 20)\n\ncreate_posterior_plot(3,3,\n                      grid_points = 20,\n                      prior = ifelse(p_grid &lt; 0.5,0,1),\n                      mtext_label = \"100% success\",\n                      xlab = \"probability water\",\n                      ylab = \"posterior probability\")"
  },
  {
    "objectID": "learn/Stat_Rethinking/ch1_3/index.html#footnotes",
    "href": "learn/Stat_Rethinking/ch1_3/index.html#footnotes",
    "title": "Chapters 1–2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\\(P(w) = E(P(w|p)) = \\int P(w|p)P(p)\\partial p\\)↩︎"
  }
]